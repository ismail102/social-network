{"cells":[{"cell_type":"code","execution_count":40,"metadata":{"execution":{"iopub.execute_input":"2023-12-04T20:18:19.796764Z","iopub.status.busy":"2023-12-04T20:18:19.796403Z","iopub.status.idle":"2023-12-04T20:18:19.809669Z","shell.execute_reply":"2023-12-04T20:18:19.808396Z","shell.execute_reply.started":"2023-12-04T20:18:19.796732Z"},"trusted":true},"outputs":[],"source":["%%capture\n","# !pip install text_hammer"]},{"cell_type":"code","execution_count":41,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-12-04T20:18:19.812166Z","iopub.status.busy":"2023-12-04T20:18:19.811699Z","iopub.status.idle":"2023-12-04T20:18:23.779602Z","shell.execute_reply":"2023-12-04T20:18:23.778572Z","shell.execute_reply.started":"2023-12-04T20:18:19.812129Z"},"trusted":true},"outputs":[],"source":["import os\n","import random\n","import torch\n","import wandb\n","import logging\n","import transformers\n","import numpy as np\n","import pandas as pd\n","import torch.nn as nn\n","import matplotlib.pyplot as plt\n","from tqdm.notebook import tqdm\n","from datasets import load_dataset\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader, Dataset\n","from sklearn import metrics, model_selection, preprocessing\n","from transformers import AdamW, get_linear_schedule_with_warmup\n","\n","import re\n","# import text_hammer as th\n","# from wordcloud import WordCloud\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","from transformers import AutoTokenizer,TFBertModel\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\",  category = FutureWarning)\n","warnings.filterwarnings(\"ignore\", category=UserWarning)\n"]},{"cell_type":"code","execution_count":42,"metadata":{},"outputs":[],"source":["# wandb.login()"]},{"cell_type":"code","execution_count":43,"metadata":{"execution":{"iopub.execute_input":"2023-12-04T20:18:26.645267Z","iopub.status.busy":"2023-12-04T20:18:26.644536Z","iopub.status.idle":"2023-12-04T20:18:26.653505Z","shell.execute_reply":"2023-12-04T20:18:26.652367Z","shell.execute_reply.started":"2023-12-04T20:18:26.645238Z"},"trusted":true},"outputs":[],"source":["def seed_everything(seed=42):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    # Some cudnn methods can be random even after fixing the seed \n","    # unless you tell it to be deterministic\n","    torch.backends.cudnn.deterministic = True\n","\n","seed_everything(1234)"]},{"cell_type":"code","execution_count":44,"metadata":{"execution":{"iopub.execute_input":"2023-12-04T20:18:26.655232Z","iopub.status.busy":"2023-12-04T20:18:26.654883Z","iopub.status.idle":"2023-12-04T20:18:26.984739Z","shell.execute_reply":"2023-12-04T20:18:26.983719Z","shell.execute_reply.started":"2023-12-04T20:18:26.655199Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(43410, 33)"]},"metadata":{},"output_type":"display_data"}],"source":["# Loading datasets into DF\n","df = pd.read_csv(\"/home/siu856533724/code/source-code/Social-Networks/Trend-Prediction/DataSet/emotion-data/goemotion_simplified.csv\")\n","\n","# Checking Shape\n","display(df.shape)"]},{"cell_type":"code","execution_count":45,"metadata":{},"outputs":[],"source":["# df1 = pd.read_csv(\"/home/siu856533724/code/source-code/Social-Networks/Trend-Prediction/DataSet/emotion-data/goemotions_1.csv\", encoding='utf8')\n","# df2 = pd.read_csv(\"/home/siu856533724/code/source-code/Social-Networks/Trend-Prediction/DataSet/emotion-data/goemotions_2.csv\", encoding='utf8')\n","# df3 = pd.read_csv(\"/home/siu856533724/code/source-code/Social-Networks/Trend-Prediction/DataSet/emotion-data/goemotions_3.csv\", encoding='utf8')\n","# df4 = pd.read_csv(\"/home/siu856533724/code/source-code/Social-Networks/Trend-Prediction/DataSet/emotion-data/reddit_emotions_processed.csv\", encoding='utf8')"]},{"cell_type":"code","execution_count":46,"metadata":{},"outputs":[],"source":["# df = pd.concat([df1, df2, df3, df4], axis=0)"]},{"cell_type":"code","execution_count":47,"metadata":{},"outputs":[],"source":["# from utils.preprocessing import expand_contractions\n","# df['text'] = df['text'].apply(expand_contractions)"]},{"cell_type":"code","execution_count":48,"metadata":{"execution":{"iopub.execute_input":"2023-12-04T20:18:26.986527Z","iopub.status.busy":"2023-12-04T20:18:26.986245Z","iopub.status.idle":"2023-12-04T20:18:27.006866Z","shell.execute_reply":"2023-12-04T20:18:27.005772Z","shell.execute_reply.started":"2023-12-04T20:18:26.986504Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0.1</th>\n","      <th>Unnamed: 0</th>\n","      <th>text</th>\n","      <th>labels</th>\n","      <th>id</th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>...</th>\n","      <th>18</th>\n","      <th>19</th>\n","      <th>20</th>\n","      <th>21</th>\n","      <th>22</th>\n","      <th>23</th>\n","      <th>24</th>\n","      <th>25</th>\n","      <th>26</th>\n","      <th>27</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>My favourite food is anything I didn't have to...</td>\n","      <td>[27]</td>\n","      <td>eebbqej</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>Now if he does off himself, everyone will thin...</td>\n","      <td>[27]</td>\n","      <td>ed00q6i</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>WHY THE FUCK IS BAYLESS ISOING</td>\n","      <td>[2]</td>\n","      <td>eezlygj</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>To make her feel threatened</td>\n","      <td>[14]</td>\n","      <td>ed7ypvh</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>4</td>\n","      <td>Dirty Southern Wankers</td>\n","      <td>[3]</td>\n","      <td>ed0bdzj</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 33 columns</p>\n","</div>"],"text/plain":["   Unnamed: 0.1  Unnamed: 0  \\\n","0             0           0   \n","1             1           1   \n","2             2           2   \n","3             3           3   \n","4             4           4   \n","\n","                                                text labels       id  0  1  2  \\\n","0  My favourite food is anything I didn't have to...   [27]  eebbqej  0  0  0   \n","1  Now if he does off himself, everyone will thin...   [27]  ed00q6i  0  0  0   \n","2                     WHY THE FUCK IS BAYLESS ISOING    [2]  eezlygj  0  0  1   \n","3                        To make her feel threatened   [14]  ed7ypvh  0  0  0   \n","4                             Dirty Southern Wankers    [3]  ed0bdzj  0  0  0   \n","\n","   3  4  ...  18  19  20  21  22  23  24  25  26  27  \n","0  0  0  ...   0   0   0   0   0   0   0   0   0   1  \n","1  0  0  ...   0   0   0   0   0   0   0   0   0   1  \n","2  0  0  ...   0   0   0   0   0   0   0   0   0   0  \n","3  0  0  ...   0   0   0   0   0   0   0   0   0   0  \n","4  1  0  ...   0   0   0   0   0   0   0   0   0   0  \n","\n","[5 rows x 33 columns]"]},"execution_count":48,"metadata":{},"output_type":"execute_result"}],"source":["df.head()"]},{"cell_type":"code","execution_count":49,"metadata":{"execution":{"iopub.execute_input":"2023-12-04T20:18:27.009096Z","iopub.status.busy":"2023-12-04T20:18:27.008314Z","iopub.status.idle":"2023-12-04T20:18:27.013124Z","shell.execute_reply":"2023-12-04T20:18:27.012162Z","shell.execute_reply.started":"2023-12-04T20:18:27.009058Z"},"trusted":true},"outputs":[],"source":["from utils.preprocessing import get_clean_dataset\n","df = get_clean_dataset(df)"]},{"cell_type":"code","execution_count":50,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0.1</th>\n","      <th>Unnamed: 0</th>\n","      <th>text</th>\n","      <th>labels</th>\n","      <th>id</th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>...</th>\n","      <th>18</th>\n","      <th>19</th>\n","      <th>20</th>\n","      <th>21</th>\n","      <th>22</th>\n","      <th>23</th>\n","      <th>24</th>\n","      <th>25</th>\n","      <th>26</th>\n","      <th>27</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>favourite food anything didnt have cook myself</td>\n","      <td>[27]</td>\n","      <td>eebbqej</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>does himself everyone will think having laugh ...</td>\n","      <td>[27]</td>\n","      <td>ed00q6i</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>FUCK BAYLESS ISOING</td>\n","      <td>[2]</td>\n","      <td>eezlygj</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>make feel threatened</td>\n","      <td>[14]</td>\n","      <td>ed7ypvh</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>4</td>\n","      <td>Dirty Southern Wankers</td>\n","      <td>[3]</td>\n","      <td>ed0bdzj</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 33 columns</p>\n","</div>"],"text/plain":["   Unnamed: 0.1  Unnamed: 0  \\\n","0             0           0   \n","1             1           1   \n","2             2           2   \n","3             3           3   \n","4             4           4   \n","\n","                                                text labels       id  0  1  2  \\\n","0     favourite food anything didnt have cook myself   [27]  eebbqej  0  0  0   \n","1  does himself everyone will think having laugh ...   [27]  ed00q6i  0  0  0   \n","2                                FUCK BAYLESS ISOING    [2]  eezlygj  0  0  1   \n","3                               make feel threatened   [14]  ed7ypvh  0  0  0   \n","4                             Dirty Southern Wankers    [3]  ed0bdzj  0  0  0   \n","\n","   3  4  ...  18  19  20  21  22  23  24  25  26  27  \n","0  0  0  ...   0   0   0   0   0   0   0   0   0   1  \n","1  0  0  ...   0   0   0   0   0   0   0   0   0   1  \n","2  0  0  ...   0   0   0   0   0   0   0   0   0   0  \n","3  0  0  ...   0   0   0   0   0   0   0   0   0   0  \n","4  1  0  ...   0   0   0   0   0   0   0   0   0   0  \n","\n","[5 rows x 33 columns]"]},"execution_count":50,"metadata":{},"output_type":"execute_result"}],"source":["df.head()"]},{"cell_type":"code","execution_count":51,"metadata":{"execution":{"iopub.execute_input":"2023-12-04T20:18:27.025380Z","iopub.status.busy":"2023-12-04T20:18:27.023365Z","iopub.status.idle":"2023-12-04T20:18:27.048001Z","shell.execute_reply":"2023-12-04T20:18:27.047231Z","shell.execute_reply.started":"2023-12-04T20:18:27.025338Z"},"trusted":true},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","train_temp, test = train_test_split(df, test_size=0.2, shuffle=True, random_state=42)"]},{"cell_type":"code","execution_count":52,"metadata":{"execution":{"iopub.execute_input":"2023-12-04T20:18:27.052148Z","iopub.status.busy":"2023-12-04T20:18:27.051802Z","iopub.status.idle":"2023-12-04T20:18:27.065886Z","shell.execute_reply":"2023-12-04T20:18:27.065107Z","shell.execute_reply.started":"2023-12-04T20:18:27.052094Z"},"trusted":true},"outputs":[],"source":["train, valid = train_test_split(train_temp, test_size=0.2, shuffle=True, random_state=42)"]},{"cell_type":"code","execution_count":53,"metadata":{"execution":{"iopub.execute_input":"2023-12-04T20:18:27.067286Z","iopub.status.busy":"2023-12-04T20:18:27.066986Z","iopub.status.idle":"2023-12-04T20:18:27.097384Z","shell.execute_reply":"2023-12-04T20:18:27.096440Z","shell.execute_reply.started":"2023-12-04T20:18:27.067260Z"},"trusted":true},"outputs":[],"source":["# train[[str(i) for i in range(0, 28)]].values.tolist()"]},{"cell_type":"markdown","metadata":{},"source":["🧹Clean Data (Noise Entity Removal) ¶"]},{"cell_type":"code","execution_count":54,"metadata":{"execution":{"iopub.execute_input":"2023-12-04T20:18:27.099256Z","iopub.status.busy":"2023-12-04T20:18:27.098829Z","iopub.status.idle":"2023-12-04T20:18:27.105209Z","shell.execute_reply":"2023-12-04T20:18:27.104177Z","shell.execute_reply.started":"2023-12-04T20:18:27.099230Z"},"trusted":true},"outputs":[],"source":["# def text_preprocessing(df,col_name):\n","#     tqdm.pandas()\n","#     df[col_name] = df[col_name].progress_apply(lambda x:str(x).lower())\n","#     df[col_name] = df[col_name].progress_apply(lambda x: th.remove_emails(x))\n","#     df[col_name] = df[col_name].progress_apply(lambda x: th.remove_html_tags(x))\n","#     df[col_name] = df[col_name].progress_apply(lambda x: th.remove_urls(x))\n","#     df[col_name] = df[col_name].progress_apply(lambda x: th.remove_special_chars(x))\n","#     df[col_name] = df[col_name].progress_apply(lambda x: th.remove_accented_chars(x))\n","#     df[col_name] = df[col_name].progress_apply(lambda text: th.cont_exp(text))\n","#     df[col_name] = df[col_name].progress_apply(lambda x: re.sub(\"[\" \n","#         u\"\\U0001F600-\\U0001F64F\"  # emoticons\n","#         u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n","#         u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n","#         u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n","#         u\"\\U00002702-\\U000027B0\"\n","#         u\"\\U000024C2-\\U0001F251\"\n","#         \"]+\", \"\", x))\n","#     df[col_name] = df[col_name].progress_apply(lambda x: re.sub(r'[^a-zA-Z]', ' ', x))\n","#     df[col_name] = df[col_name].progress_apply(lambda x: ' '.join(x.split()))\n","    \n","#     return(df)"]},{"cell_type":"code","execution_count":55,"metadata":{"execution":{"iopub.execute_input":"2023-12-04T20:18:27.107234Z","iopub.status.busy":"2023-12-04T20:18:27.106552Z","iopub.status.idle":"2023-12-04T20:18:27.117680Z","shell.execute_reply":"2023-12-04T20:18:27.116799Z","shell.execute_reply.started":"2023-12-04T20:18:27.107192Z"},"trusted":true},"outputs":[],"source":["# # %%capture\n","# train = text_preprocessing(train,'text')\n","# valid = text_preprocessing(valid,'text')\n","# test = text_preprocessing(test,'text')"]},{"cell_type":"markdown","metadata":{},"source":["🗑️Remove Stop Words ¶"]},{"cell_type":"code","execution_count":56,"metadata":{"execution":{"iopub.execute_input":"2023-12-04T20:18:27.119312Z","iopub.status.busy":"2023-12-04T20:18:27.118889Z","iopub.status.idle":"2023-12-04T20:18:27.127278Z","shell.execute_reply":"2023-12-04T20:18:27.126447Z","shell.execute_reply.started":"2023-12-04T20:18:27.119277Z"},"trusted":true},"outputs":[],"source":["# stop = stopwords.words('english')\n","# train['text'] = train['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n","# valid['text'] = valid['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n","# test['text'] = test['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))"]},{"cell_type":"markdown","metadata":{},"source":["🌱Lemmatization ¶"]},{"cell_type":"code","execution_count":57,"metadata":{"execution":{"iopub.execute_input":"2023-12-04T20:18:27.128781Z","iopub.status.busy":"2023-12-04T20:18:27.128375Z","iopub.status.idle":"2023-12-04T20:18:27.136140Z","shell.execute_reply":"2023-12-04T20:18:27.135188Z","shell.execute_reply.started":"2023-12-04T20:18:27.128755Z"},"trusted":true},"outputs":[],"source":["# nltk.download('wordnet')\n","# !unzip \"/usr/share/nltk_data/corpora/wordnet.zip\" -d \"/usr/share/nltk_data/corpora/\""]},{"cell_type":"code","execution_count":58,"metadata":{"execution":{"iopub.execute_input":"2023-12-04T20:18:27.137637Z","iopub.status.busy":"2023-12-04T20:18:27.137351Z","iopub.status.idle":"2023-12-04T20:18:27.144847Z","shell.execute_reply":"2023-12-04T20:18:27.143738Z","shell.execute_reply.started":"2023-12-04T20:18:27.137613Z"},"trusted":true},"outputs":[],"source":["# def word_lemmatizer(text):\n","#     lemmatizer = WordNetLemmatizer()\n","#     return ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n","\n","# train['text'] = train['text'].apply(lambda text: word_lemmatizer(text))\n","# valid['text'] = valid['text'].apply(lambda text: word_lemmatizer(text))"]},{"cell_type":"code","execution_count":59,"metadata":{},"outputs":[],"source":["def get_class_numbers():\n","    return [str(i) for i in range(28)]"]},{"cell_type":"code","execution_count":60,"metadata":{"execution":{"iopub.execute_input":"2023-12-04T20:18:27.182002Z","iopub.status.busy":"2023-12-04T20:18:27.181686Z","iopub.status.idle":"2023-12-04T20:18:27.194021Z","shell.execute_reply":"2023-12-04T20:18:27.192911Z","shell.execute_reply.started":"2023-12-04T20:18:27.181978Z"},"trusted":true},"outputs":[],"source":["from transformers import AutoModelForSequenceClassification\n","from utils.dataset import Dataset\n","\n","def build_dataset(tokenizer_max_len, tokenizer):\n","    train_dataset = Dataset(train.text.tolist(), train[get_class_numbers()].values.tolist(), tokenizer, tokenizer_max_len)\n","    valid_dataset = Dataset(valid.text.tolist(), valid[get_class_numbers()].values.tolist(), tokenizer, tokenizer_max_len)\n","    test_dataset = Dataset(test.text.tolist(), test[get_class_numbers()].values.tolist(), tokenizer, tokenizer_max_len)\n","\n","    return train_dataset, valid_dataset, test_dataset\n","    # return train_dataset, valid_dataset\n","\n","def build_dataloader(train_dataset, valid_dataset, test_dataset, batch_size):\n","    train_data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n","    valid_data_loader = DataLoader(valid_dataset, batch_size=int(batch_size/2), shuffle=True, num_workers=1)\n","    test_data_loader = DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=True, num_workers=1)\n","\n","    return train_data_loader, valid_data_loader, test_data_loader\n","    # return train_data_loader, valid_data_loader\n","\n","def ret_model(do_prob, model_ckpt, n_labels, is_freez):\n","    # model = TextClassification(n_classes=n_labels, dropout=do_prob, model_ckpt=model_ckpt)\n","    model = AutoModelForSequenceClassification.from_pretrained(model_ckpt, num_labels=n_labels)\n","    if model_ckpt != 'roberta-large' and is_freez==True:\n","        # Freeze all layers except the classification layer\n","        for param in model.base_model.parameters():\n","            param.requires_grad = False\n","\n","        # Modify the classification layer for the new task (target domain)\n","        model.classifier = torch.nn.Linear(model.classifier.in_features, n_labels)\n","        \n","    return model"]},{"cell_type":"code","execution_count":61,"metadata":{"execution":{"iopub.execute_input":"2023-12-04T20:18:27.195699Z","iopub.status.busy":"2023-12-04T20:18:27.195397Z","iopub.status.idle":"2023-12-04T20:18:27.203501Z","shell.execute_reply":"2023-12-04T20:18:27.202589Z","shell.execute_reply.started":"2023-12-04T20:18:27.195667Z"},"trusted":true},"outputs":[],"source":["def ret_optimizer(model):\n","    param_optimizer = list(model.named_parameters())\n","    no_decay = [\"bias\", \"LayerNorm.bias\"]\n","    optimizer_parameters = [\n","        {\n","            \"params\": [\n","                p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n","            ],\n","            \"weight_decay\": 0.001,\n","        },\n","        {\n","            \"params\": [\n","                p for n, p in param_optimizer if any(nd in n for nd in no_decay)\n","            ],\n","            \"weight_decay\": 0.0,\n","        },\n","    ]\n","    # opt = AdamW(optimizer_parameters, lr=wandb.config.learning_rate)\n","    opt = AdamW(model.parameters(), lr=wandb.config.learning_rate)\n","    return opt"]},{"cell_type":"code","execution_count":62,"metadata":{"execution":{"iopub.execute_input":"2023-12-04T20:18:27.205109Z","iopub.status.busy":"2023-12-04T20:18:27.204749Z","iopub.status.idle":"2023-12-04T20:18:27.215226Z","shell.execute_reply":"2023-12-04T20:18:27.214300Z","shell.execute_reply.started":"2023-12-04T20:18:27.205077Z"},"trusted":true},"outputs":[],"source":["def ret_scheduler(optimizer, num_train_steps):\n","    scheduler  = get_linear_schedule_with_warmup(\n","        optimizer, num_warmup_steps=0, num_training_steps=num_train_steps)\n","    return scheduler "]},{"cell_type":"code","execution_count":63,"metadata":{"execution":{"iopub.execute_input":"2023-12-04T20:18:27.216787Z","iopub.status.busy":"2023-12-04T20:18:27.216454Z","iopub.status.idle":"2023-12-04T20:18:27.225632Z","shell.execute_reply":"2023-12-04T20:18:27.224589Z","shell.execute_reply.started":"2023-12-04T20:18:27.216759Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn.functional as F\n","\n","def loss_fn(y_pred, y_true, loss_func, alpha=0.25, gamma=2.0):\n","    \n","    if y_true is None:\n","        return None\n","    \n","    if loss_func == 'cross_entropy':\n","        return F.cross_entropy(y_pred, y_true.float())\n","    if loss_func == 'focal_loss':\n","        logpt = -F.binary_cross_entropy_with_logits(y_pred, y_true.float())\n","        pt = torch.exp(logpt)\n","        loss = -((1 - pt) ** gamma) * logpt\n","        return loss.mean()\n","    if loss_func == 'hinge_loss':\n","        return F.hinge_embedding_loss(y_pred, y_true.float())\n","    if loss_func == 'mse_loss':\n","         return nn.MSELoss()(y_pred, y_true.float())\n","    \n","    if loss_func == 'multi_label_soft_margin':\n","        return nn.MultiLabelSoftMarginLoss()(y_pred, y_true.float())\n","    \n","    # if not all others\n","    return nn.BCEWithLogitsLoss()(y_pred, y_true.float())"]},{"cell_type":"code","execution_count":64,"metadata":{"execution":{"iopub.execute_input":"2023-12-04T20:18:27.237045Z","iopub.status.busy":"2023-12-04T20:18:27.236723Z","iopub.status.idle":"2023-12-04T20:18:27.255078Z","shell.execute_reply":"2023-12-04T20:18:27.254066Z","shell.execute_reply.started":"2023-12-04T20:18:27.237008Z"},"trusted":true},"outputs":[],"source":["def train_fn(data_loader, model, optimizer, device, scheduler):\n","    train_loss = 0.0\n","    model.train()\n","    for bi, d in tqdm(enumerate(data_loader), total=len(data_loader)):\n","        ids = d[\"ids\"]\n","        mask = d[\"mask\"]\n","        targets = d[\"labels\"]\n","        # token_type_ids = d['token_type_ids']\n","\n","        ids = ids.to(device, dtype=torch.long)\n","        mask = mask.to(device, dtype=torch.long)\n","        targets = targets.to(device, dtype=torch.float)\n","        \n","        optimizer.zero_grad()\n","        torch.cuda.empty_cache()\n","        outputs = model(ids, attention_mask=mask, labels=targets)\n","        # loss = outputs.loss\n","\n","        loss = loss_fn(y_pred=outputs.logits, y_true=targets, \n","                           loss_func=get_loss_func())\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","        optimizer.step()\n","        scheduler.step()\n","        \n","        train_loss += loss.item()\n","        \n","    return train_loss\n","    \n","\n","def eval_fn(data_loader, model, device):\n","    eval_loss = 0.0\n","    model.eval()\n","    fin_targets = []\n","    fin_outputs = []\n","    val_accuracy = 0\n","    with torch.no_grad():\n","        for bi, d in tqdm(enumerate(data_loader), total=len(data_loader)):\n","            ids = d[\"ids\"]\n","            mask = d[\"mask\"]\n","            targets = d[\"labels\"]\n","            # token_type_ids = d['token_type_ids']\n","\n","            ids = ids.to(device, dtype=torch.long)\n","            mask = mask.to(device, dtype=torch.long)\n","            targets = targets.to(device, dtype=torch.float)\n","\n","            outputs = model(ids, attention_mask=mask, labels=targets)\n","            loss = loss_fn(y_pred=outputs.logits, y_true=targets, \n","                           loss_func=get_loss_func())\n","\n","            eval_loss += loss.item()\n","            \n","            fin_targets.extend(targets)\n","            fin_outputs.extend(torch.nn.functional.softmax(outputs.logits))\n","            \n","    return eval_loss, fin_outputs, fin_targets\n","\n","def test_fn(data_loader, model, device):\n","    test_loss = 0.0\n","    model.eval()\n","    fin_targets = []\n","    fin_outputs = []\n","    test_acc = 0\n","    with torch.no_grad():\n","        for bi, d in tqdm(enumerate(data_loader), total=len(data_loader)):\n","            ids = d[\"ids\"]\n","            mask = d[\"mask\"]\n","            targets = d[\"labels\"]\n","            # token_type_ids = d['token_type_ids']\n","\n","            ids = ids.to(device, dtype=torch.long)\n","            mask = mask.to(device, dtype=torch.long)\n","            targets = targets.to(device, dtype=torch.float)\n","\n","            outputs = model(ids, attention_mask=mask, labels=targets)\n","            loss = loss_fn(y_pred=outputs.logits, y_true=targets, \n","                           loss_func=get_loss_func())\n","#             logits = outputs.logits\n","            \n","            test_loss += loss.item()\n","            \n","            fin_targets.extend(targets)\n","            fin_outputs.extend(torch.nn.functional.softmax(outputs.logits))\n","\n","    return test_loss, fin_outputs, fin_targets"]},{"cell_type":"code","execution_count":65,"metadata":{"execution":{"iopub.execute_input":"2023-12-04T20:18:27.256507Z","iopub.status.busy":"2023-12-04T20:18:27.256233Z","iopub.status.idle":"2023-12-04T20:18:27.266033Z","shell.execute_reply":"2023-12-04T20:18:27.265269Z","shell.execute_reply.started":"2023-12-04T20:18:27.256484Z"},"trusted":true},"outputs":[],"source":["# n_labels = 8\n","def model_info(model_ckpt):\n","    tokenizer = transformers.AutoTokenizer.from_pretrained(model_ckpt, do_lower_case=True)\n","    bert_model = transformers.AutoModel.from_pretrained(model_ckpt)\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    return bert_model, tokenizer, device"]},{"cell_type":"code","execution_count":66,"metadata":{"execution":{"iopub.execute_input":"2023-12-04T20:18:27.267392Z","iopub.status.busy":"2023-12-04T20:18:27.267073Z","iopub.status.idle":"2023-12-04T20:18:27.275065Z","shell.execute_reply":"2023-12-04T20:18:27.274177Z","shell.execute_reply.started":"2023-12-04T20:18:27.267369Z"},"trusted":true},"outputs":[],"source":["# using time module\n","import time\n","from datetime import datetime  \n","\n","current_time = datetime.now()\n","time_stamp = current_time.timestamp()\n","\n","date_time = datetime.fromtimestamp(time_stamp)\n","str_date_time = date_time.strftime(\"%d_%m_%Y-%H_%M_%S\")\n","\n","# dir = './bert_model_save'\n","output_dir = './model_save/emotion_pred/BERT/'\n","\n","# Create output directory if needed\n","if not os.path.exists(output_dir):\n","    os.makedirs(output_dir)"]},{"cell_type":"code","execution_count":67,"metadata":{"execution":{"iopub.execute_input":"2023-12-04T20:18:27.276508Z","iopub.status.busy":"2023-12-04T20:18:27.276209Z","iopub.status.idle":"2023-12-04T20:18:27.289238Z","shell.execute_reply":"2023-12-04T20:18:27.288302Z","shell.execute_reply.started":"2023-12-04T20:18:27.276484Z"},"trusted":true},"outputs":[],"source":["from utils.common_functions import log_metrics\n","\n","training_stats = []\n","test_stats = []\n","def trainer(config=None):\n","    with wandb.init(config=config):\n","        config = wandb.config\n","\n","        _, tokenizer, device = model_info(get_model_ckpt())\n","\n","        train_dataset, valid_dataset, test_dataset = build_dataset(config.tokenizer_max_len, tokenizer)\n","        train_data_loader, valid_data_loader, test_data_loader = build_dataloader(train_dataset, valid_dataset, test_dataset, config.batch_size)\n","        \n","        print(\"Length of Train Dataloader: \", len(train_data_loader))\n","        print(\"Length of Valid Dataloader: \", len(valid_data_loader))\n","#         print(\"Length of Test Dataloader: \", len(test_data_loader))\n","\n","        n_train_steps = int(len(train_dataset) / config.batch_size * 10)\n","\n","        model = ret_model(config.dropout, get_model_ckpt(), get_n_labels(), is_freez=get_Freeze())\n","        optimizer = ret_optimizer(model)\n","        scheduler = ret_scheduler(optimizer, n_train_steps)\n","        model.to(device)\n","        # model = nn.parallel.DistributedDataParallel(model)\n","        wandb.watch(model)\n","        \n","        n_epochs = config.epochs\n","\n","        best_val_loss = 100\n","        for epoch in tqdm(range(n_epochs)):\n","            train_loss = train_fn(train_data_loader, model, optimizer, device, scheduler)\n","            eval_loss, preds, labels = eval_fn(valid_data_loader, model, device)\n","          \n","            metrics = log_metrics(preds, labels)\n","            \n","            avg_train_loss, avg_val_loss = train_loss / len(train_data_loader), eval_loss / len(valid_data_loader)\n","\n","            wandb.log({\n","                \"epoch\": epoch + 1,\n","                \"train_loss\": avg_train_loss,\n","                \"val_loss\": avg_val_loss,\n","                \"auc_score\": metrics['auc'],\n","            })\n","\n","            print(\"AUC score: \", metrics['auc'])\n","            print(\"Average Train loss: \", avg_train_loss)\n","            print(\"Average Valid loss: \", avg_val_loss)\n","            print(\"Valid F1: \", metrics['f1'])\n","            print(\"Valid Acc: \", metrics['acc'])\n","\n","            training_stats.append(\n","            {\n","                'Batch': config.batch_size,\n","                'Max Token': config.tokenizer_max_len,\n","                'Model':  get_model_ckpt(),\n","                'LR': config.learning_rate,\n","                'Loss func': get_loss_func(),\n","                'Epoch': epoch + 1,\n","                'Train Loss': avg_train_loss,\n","                'Valid. Loss': avg_val_loss,\n","                'Valid. AUC': metrics['auc'],\n","                'F1 score': metrics['f1'],\n","                'Recall': metrics['recall'],\n","                'Precision': metrics['precision'],\n","                'Accuracy': metrics['acc'],\n","            })\n","\n","            if avg_val_loss < best_val_loss:\n","                best_val_loss = avg_val_loss\n","                torch.save(model, output_dir+f\"best_{get_type()}_{get_model_ckpt()}-{str(str_date_time)}.pt\")\n","                tokenizer.save_pretrained(output_dir)\n","                print(\"Model saved as current val_loss is: \", best_val_loss)\n","            \n","        test_loss, preds, labels = test_fn(test_data_loader, model, device)\n","        metrics = log_metrics(preds, labels)\n","        avg_test_loss = test_loss / len(test_data_loader)\n","        print(\"Test loss: \", avg_test_loss)\n","        print(\"AUC: \", metrics['auc'])\n","        print(\"Test F1: \", metrics['f1'])\n","        print(\"Test Recall: \", metrics['recall'])\n","        print(\"Test Precision: \", metrics['precision'])\n","        print(\"Test Acc: \", metrics['acc'])\n","        test_stats.append({\n","                'Batch': config.batch_size,\n","                'Max Token': config.tokenizer_max_len,\n","                'Model':  get_model_ckpt(),\n","                'LR': config.learning_rate,\n","                'Loss func': get_loss_func(),\n","                'Test. Loss': avg_test_loss,\n","                'Test. AUC': metrics['auc'],\n","                'F1 score': metrics['f1'],\n","                'Recall': metrics['recall'],\n","                'Precision': metrics['precision'],\n","                'Accuracy': metrics['acc']\n","                })"]},{"cell_type":"code","execution_count":68,"metadata":{"execution":{"iopub.execute_input":"2023-12-04T20:18:27.295050Z","iopub.status.busy":"2023-12-04T20:18:27.294769Z","iopub.status.idle":"2023-12-04T20:18:27.301123Z","shell.execute_reply":"2023-12-04T20:18:27.300284Z","shell.execute_reply.started":"2023-12-04T20:18:27.295027Z"},"trusted":true},"outputs":[],"source":["def get_project():\n","    return 'emotion-prediction'"]},{"cell_type":"code","execution_count":69,"metadata":{"execution":{"iopub.execute_input":"2023-12-04T20:18:27.302433Z","iopub.status.busy":"2023-12-04T20:18:27.302097Z","iopub.status.idle":"2023-12-04T20:18:30.214955Z","shell.execute_reply":"2023-12-04T20:18:30.213932Z","shell.execute_reply.started":"2023-12-04T20:18:27.302409Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Create sweep with ID: annlix9l\n","Sweep URL: https://wandb.ai/supreme-lab/emotion-prediction/sweeps/annlix9l\n"]}],"source":["sweep_config = {\n","    'method': 'grid', #grid, random, bayesian\n","    'metric': {\n","      'name': 'auc_score',\n","      'goal': 'maximize'   \n","    },\n","    'parameters': {\n","        'learning_rate': {\n","            'values': [2e-05, 3e-05, 5e-05]\n","        },\n","        'batch_size': {\n","            'values': [16]\n","        },\n","        'epochs':{'value': 3},\n","        'dropout':{\n","            'values': [0.3]\n","        },\n","        'tokenizer_max_len': {\n","            'values': [30, 70]\n","        },\n","    }\n","}\n","\n","sweep_id = wandb.sweep(sweep_config, project=get_project())"]},{"cell_type":"code","execution_count":70,"metadata":{"execution":{"iopub.execute_input":"2023-12-04T20:18:30.216763Z","iopub.status.busy":"2023-12-04T20:18:30.216184Z","iopub.status.idle":"2023-12-04T20:18:30.224282Z","shell.execute_reply":"2023-12-04T20:18:30.223230Z","shell.execute_reply.started":"2023-12-04T20:18:30.216736Z"},"trusted":true},"outputs":[],"source":["def get_Freeze():\n","    return False\n","\n","def get_model_ckpt():\n","    #  return \"bsingh/roberta_goEmotion\"\n","    #  return 'distilbert-base-uncased'\n","    return 'bert-base-uncased'\n","    # return 'bert-large-uncased'\n","    #  return 'cardiffnlp/twitter-roberta-base-emotion-multilabel-latest'\n","    #  return 'roberta-large'\n","\n","def get_loss_func():\n","    # return 'hinge_loss'\n","    # return 'focal_loss'\n","    # return 'cross_entropy'\n","    # return 'mse_loss'\n","    # return 'multi_label_soft_margin'\n","    return 'bce_loss'\n","\n","def get_classes():\n","    trends = [\"approval\",\"toxic\",\"obscene\", 'insult', \"threat\", \"hate\", \"offensive\", \"neither\"]\n","    emotions = [\n","        'admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring',\n","        'confusion', 'curiosity', 'desire', 'disappointment', 'disapproval',\n","        'disgust', 'embarrassment', 'excitement', 'fear', 'gratitude', 'grief',\n","        'joy', 'love', 'nervousness', 'optimism', 'pride', 'realization',\n","        'relief', 'remorse', 'sadness', 'surprise', 'neutral'\n","    ]\n","    # return range(0, 28)\n","    return emotions\n","def get_n_labels():\n","    return len(get_classes())\n","\n","def get_type():\n","    return 'emotion'\n","#     return 'emotion'"]},{"cell_type":"code","execution_count":71,"metadata":{"execution":{"iopub.execute_input":"2023-12-04T20:18:30.225821Z","iopub.status.busy":"2023-12-04T20:18:30.225530Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: wzh3x5nr with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 3\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 2e-05\n","\u001b[34m\u001b[1mwandb\u001b[0m: \ttokenizer_max_len: 30\n","Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"]},{"name":"stdout","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"data":{"text/html":["Tracking run with wandb version 0.13.10"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["Length of Train Dataloader:  1737\n","Length of Valid Dataloader:  869\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1adad13929424edc84b5a8424d700118","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"82108749246a405baf273e1cdf82ea82","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1737 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7b6a5d1445c3452f979b21c8a10a3d1f","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/869 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["AUC score:  0.8867683999831188\n","Average Train loss:  0.15360863348946327\n","Average Valid loss:  0.11686379228846114\n","Valid F1:  0.7300484101084144\n","Valid Acc:  0.9596376126033482\n","Model saved as current val_loss is:  0.11686379228846114\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"137f788e86d6432eb0739b900546a9a4","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1737 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"de4fa6c6af2f4f829a2b4c1708bb34f8","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/869 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["AUC score:  0.907486221726261\n","Average Train loss:  0.11070345003301547\n","Average Valid loss:  0.10467332896269575\n","Valid F1:  0.7555082094416683\n","Valid Acc:  0.9634373328945744\n","Model saved as current val_loss is:  0.10467332896269575\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9eab86996d604c49bd6c32f8e3f2f7c1","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1737 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"92423b3eeeb7463fb2e3117f31329d91","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/869 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["AUC score:  0.9102913934604182\n","Average Train loss:  0.09727989689210537\n","Average Valid loss:  0.10255592962849126\n","Valid F1:  0.7587822361512697\n","Valid Acc:  0.9639155114968533\n","Model saved as current val_loss is:  0.10255592962849126\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ac7f0cd2688c4e488fb97abbd42264c1","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Test loss:  0.10309625416994095\n","AUC:  0.911174378674471\n","Test F1:  0.7533010868917223\n","Test Recall:  0.7351300761013895\n","Test Precision:  0.7749915664632572\n","Test Acc:  0.9630804291308783\n"]},{"data":{"text/html":["Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>auc_score</td><td>▁▇█</td></tr><tr><td>epoch</td><td>▁▅█</td></tr><tr><td>train_loss</td><td>█▃▁</td></tr><tr><td>val_loss</td><td>█▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>auc_score</td><td>0.91029</td></tr><tr><td>epoch</td><td>3</td></tr><tr><td>train_loss</td><td>0.09728</td></tr><tr><td>val_loss</td><td>0.10256</td></tr></table><br/></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["You can sync this run to the cloud by running:<br/><code>wandb sync /home/siu856533724/code/source-code/Social-Networks/Trend-Prediction/Main-Implementation/wandb/offline-run-20231229_195226-wzh3x5nr<code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/offline-run-20231229_195226-wzh3x5nr/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: e3lg6dqq with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 3\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 2e-05\n","\u001b[34m\u001b[1mwandb\u001b[0m: \ttokenizer_max_len: 70\n","Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"]},{"name":"stdout","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"data":{"text/html":["Tracking run with wandb version 0.13.10"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["Length of Train Dataloader:  1737\n","Length of Valid Dataloader:  869\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f9d0758fac844a01ae9a5f1d810e5be3","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ddb0f1758bad47ecaa47f2bfbbf13b01","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1737 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"47b603afb97e46bab7b271742350ef95","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/869 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["AUC score:  0.8870138643468259\n","Average Train loss:  0.1489311983196572\n","Average Valid loss:  0.11508781447903745\n","Valid F1:  0.7334185063898634\n","Valid Acc:  0.9601414997326313\n","Model saved as current val_loss is:  0.11508781447903745\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f7740967d0134f07ba32a4e7d7245501","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1737 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2ea9b106974a491d89952e06caeaa51e","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/869 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["AUC score:  0.9099094944042758\n","Average Train loss:  0.10949707897052932\n","Average Valid loss:  0.10536041419408675\n","Valid F1:  0.7516702641587008\n","Valid Acc:  0.9628563201842787\n","Model saved as current val_loss is:  0.10536041419408675\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7c9e3870a9124391947a5a9f22c7f09e","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1737 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"61264a35d3d44b7bbab13ac2715a82f7","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/869 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["AUC score:  0.9107552507502299\n","Average Train loss:  0.09642304256386845\n","Average Valid loss:  0.10292247077075055\n","Valid F1:  0.7543198718017858\n","Valid Acc:  0.9632316646785406\n","Model saved as current val_loss is:  0.10292247077075055\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"db4d81bf6c3d4d5788147ccebda53b4c","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Test loss:  0.10314390063285828\n","AUC:  0.9120252652254311\n","Test F1:  0.752441815158901\n","Test Recall:  0.7344031376838317\n","Test Precision:  0.7739593182792622\n","Test Acc:  0.9629405666897028\n"]},{"data":{"text/html":["Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>auc_score</td><td>▁██</td></tr><tr><td>epoch</td><td>▁▅█</td></tr><tr><td>train_loss</td><td>█▃▁</td></tr><tr><td>val_loss</td><td>█▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>auc_score</td><td>0.91076</td></tr><tr><td>epoch</td><td>3</td></tr><tr><td>train_loss</td><td>0.09642</td></tr><tr><td>val_loss</td><td>0.10292</td></tr></table><br/></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["You can sync this run to the cloud by running:<br/><code>wandb sync /home/siu856533724/code/source-code/Social-Networks/Trend-Prediction/Main-Implementation/wandb/offline-run-20231229_195818-e3lg6dqq<code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/offline-run-20231229_195818-e3lg6dqq/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: jhokkzci with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 3\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 3e-05\n","\u001b[34m\u001b[1mwandb\u001b[0m: \ttokenizer_max_len: 30\n","Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"]},{"name":"stdout","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"data":{"text/html":["Tracking run with wandb version 0.13.10"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["Length of Train Dataloader:  1737\n","Length of Valid Dataloader:  869\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8de19e840fe4402ea06b900ddc22756b","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"01eb0a67240a4ba88f286ed4b6d5083b","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1737 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"32e74ea2ebae4700b8394d9caeac7b9b","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/869 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["AUC score:  0.8988020461303201\n","Average Train loss:  0.14064889688154164\n","Average Valid loss:  0.11058520839921487\n","Valid F1:  0.7442165699855269\n","Valid Acc:  0.9617559952284974\n","Model saved as current val_loss is:  0.11058520839921487\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2ec38c8680034162bd8bc8dbcfffdcec","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1737 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3d7fb93b845f441688c6e5307033f214","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/869 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","AUC score:  0.915195498217092\n","Average Train loss:  0.10479289197208007\n","Average Valid loss:  0.10224175219551604\n","Valid F1:  0.757075498535936\n","Valid Acc:  0.9636532845214101\n","Model saved as current val_loss is:  0.10224175219551604\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9e423f4f602b4ff0873ed64cd5a56975","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1737 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5f61c69b7e9046c4a96207d273a005f6","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/869 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["AUC score:  0.9106062694454125\n","Average Train loss:  0.08959937900750886\n","Average Valid loss:  0.10363262664779627\n","Valid F1:  0.7503019852364046\n","Valid Acc:  0.9625735263872321\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"53bbaf1f556a4d74aaa55b06054cdc2f","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Test loss:  0.10411184281110764\n","AUC:  0.910166248208662\n","Test F1:  0.7490271609366461\n","Test Recall:  0.7316269239082802\n","Test Precision:  0.7696997641529446\n","Test Acc:  0.9623646625201566\n"]},{"data":{"text/html":["Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>auc_score</td><td>▁█▆</td></tr><tr><td>epoch</td><td>▁▅█</td></tr><tr><td>train_loss</td><td>█▃▁</td></tr><tr><td>val_loss</td><td>█▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>auc_score</td><td>0.91061</td></tr><tr><td>epoch</td><td>3</td></tr><tr><td>train_loss</td><td>0.0896</td></tr><tr><td>val_loss</td><td>0.10363</td></tr></table><br/></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["You can sync this run to the cloud by running:<br/><code>wandb sync /home/siu856533724/code/source-code/Social-Networks/Trend-Prediction/Main-Implementation/wandb/offline-run-20231229_200606-jhokkzci<code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/offline-run-20231229_200606-jhokkzci/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: orcmomht with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 3\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 3e-05\n","\u001b[34m\u001b[1mwandb\u001b[0m: \ttokenizer_max_len: 70\n","Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"]},{"name":"stdout","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"data":{"text/html":["Tracking run with wandb version 0.13.10"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["Length of Train Dataloader:  1737\n","Length of Valid Dataloader:  869\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1d9239e3f3314eb99307860f16b5d6a8","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3570126ed96444989ba2289a324ee450","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1737 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b1516d5aa2574c47861802d5b299b304","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/869 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["AUC score:  0.8947521842559564\n","Average Train loss:  0.14339366586955032\n","Average Valid loss:  0.11166622050715261\n","Valid F1:  0.7390582593506558\n","Valid Acc:  0.9609847394183703\n","Model saved as current val_loss is:  0.11166622050715261\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"78f7eb7890ca4f40beb2d5d6273ec8f1","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1737 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b4d1e01ff7a54e53bfae1ab6049e167e","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/869 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["AUC score:  0.9116832992414214\n","Average Train loss:  0.10572485838709511\n","Average Valid loss:  0.10347447782686309\n","Valid F1:  0.7557953428271609\n","Valid Acc:  0.9634733248323804\n","Model saved as current val_loss is:  0.10347447782686309\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2456b993f3f844559b8494f7127f38c6","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1737 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cd86522d9e4a4b7c915b94bc219bf45c","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/869 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["AUC score:  0.9106798848099817\n","Average Train loss:  0.09102772597049802\n","Average Valid loss:  0.10347903886396262\n","Valid F1:  0.7501547041270646\n","Valid Acc:  0.9625940932088355\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5d4d97b5d96846789a7a3f4acde225ad","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Test loss:  0.1041160523891449\n","AUC:  0.9110798318312251\n","Test F1:  0.7450868481221038\n","Test Recall:  0.7278273942560768\n","Test Precision:  0.7656271161564041\n","Test Acc:  0.9617969855530325\n"]},{"data":{"text/html":["Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>auc_score</td><td>▁██</td></tr><tr><td>epoch</td><td>▁▅█</td></tr><tr><td>train_loss</td><td>█▃▁</td></tr><tr><td>val_loss</td><td>█▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>auc_score</td><td>0.91068</td></tr><tr><td>epoch</td><td>3</td></tr><tr><td>train_loss</td><td>0.09103</td></tr><tr><td>val_loss</td><td>0.10348</td></tr></table><br/></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["You can sync this run to the cloud by running:<br/><code>wandb sync /home/siu856533724/code/source-code/Social-Networks/Trend-Prediction/Main-Implementation/wandb/offline-run-20231229_201044-orcmomht<code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/offline-run-20231229_201044-orcmomht/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: zzicliqp with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 3\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5e-05\n","\u001b[34m\u001b[1mwandb\u001b[0m: \ttokenizer_max_len: 30\n","Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"]},{"name":"stdout","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"data":{"text/html":["Tracking run with wandb version 0.13.10"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["Length of Train Dataloader:  1737\n","Length of Valid Dataloader:  869\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b1ef3f5a2e18429e98b5ee232bb4c804","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bd5df8b6f018400497bda6fa4083c307","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1737 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ce50d9fb85d8473883c3c186e9e9f52c","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/869 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["AUC score:  0.9066583296492194\n","Average Train loss:  0.13215114438351916\n","Average Valid loss:  0.10733744956904506\n","Valid F1:  0.747375649185291\n","Valid Acc:  0.9622187487145737\n","Model saved as current val_loss is:  0.10733744956904506\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"215a23af2b48476784d046a82aaf6887","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1737 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d46239070c2a491d942eb28e4f7d2580","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/869 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["AUC score:  0.9123033799121997\n","Average Train loss:  0.10203526732680711\n","Average Valid loss:  0.1031793593820206\n","Valid F1:  0.7536577739986443\n","Valid Acc:  0.9631442556867261\n","Model saved as current val_loss is:  0.1031793593820206\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"96df9c1beeac4e49b22b96a10fadeb86","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1737 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f66f15d96c0549d9bf95f0b472f8aefb","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/869 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["AUC score:  0.9112299805024772\n","Average Train loss:  0.08620042226582371\n","Average Valid loss:  0.10436672415965857\n","Valid F1:  0.7528031392246373\n","Valid Acc:  0.9629437291760932\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"56c42c9ab1b04b1f934091ae2269eb67","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Test loss:  0.10463949292898178\n","AUC:  0.9105992438011585\n","Test F1:  0.7529380069951909\n","Test Recall:  0.7355091367031601\n","Test Precision:  0.7735867887831689\n","Test Acc:  0.9629117714812255\n"]},{"data":{"text/html":["Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>auc_score</td><td>▁█▇</td></tr><tr><td>epoch</td><td>▁▅█</td></tr><tr><td>train_loss</td><td>█▃▁</td></tr><tr><td>val_loss</td><td>█▁▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>auc_score</td><td>0.91123</td></tr><tr><td>epoch</td><td>3</td></tr><tr><td>train_loss</td><td>0.0862</td></tr><tr><td>val_loss</td><td>0.10437</td></tr></table><br/></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["You can sync this run to the cloud by running:<br/><code>wandb sync /home/siu856533724/code/source-code/Social-Networks/Trend-Prediction/Main-Implementation/wandb/offline-run-20231229_201825-zzicliqp<code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/offline-run-20231229_201825-zzicliqp/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: vp8vodi2 with config:\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 3\n","\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5e-05\n","\u001b[34m\u001b[1mwandb\u001b[0m: \ttokenizer_max_len: 70\n","Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"]},{"name":"stdout","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"data":{"text/html":["Tracking run with wandb version 0.13.10"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["Length of Train Dataloader:  1737\n","Length of Valid Dataloader:  869\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5b47d9bfe0ba4644810162d431bfdf46","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2196358de9ae4d7d95e08c24325112b4","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1737 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2a2e98e66675458cb1528ef0b90b6bc1","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/869 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["AUC score:  0.9023937573882199\n","Average Train loss:  0.13124333431426313\n","Average Valid loss:  0.10786155237890223\n","Valid F1:  0.7453998440325629\n","Valid Acc:  0.9619256715067254\n","Model saved as current val_loss is:  0.10786155237890223\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2aacc8a40d5c4174836cd6a32b6143c1","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1737 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n"]},{"name":"stdout","output_type":"stream","text":["\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"31da9c2ec6d14607b5a5aa6c69eba602","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/869 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","AUC score:  0.9128129932551201\n","Average Train loss:  0.10179455122584986\n","Average Valid loss:  0.10263054361791962\n","Valid F1:  0.7544326485420959\n","Valid Acc:  0.9632625149109456\n","Model saved as current val_loss is:  0.10263054361791962\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4a3d967e619b4a2092f69fe6c4ab90bd","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1737 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9b640a06113b45f197c94451daabf718","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/869 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["AUC score:  0.9141085717807652\n","Average Train loss:  0.0854949074087983\n","Average Valid loss:  0.1043768652786594\n","Valid F1:  0.7508656499881212\n","Valid Acc:  0.9626249434412406\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c96da70457c849f19232539097625a1f","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Test loss:  0.10489863157272339\n","AUC:  0.913872085363516\n","Test F1:  0.7488195282351071\n","Test Recall:  0.7317686710544002\n","Test Precision:  0.7690049129059349\n","Test Acc:  0.9622782768947247\n"]},{"data":{"text/html":["Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>auc_score</td><td>▁▇█</td></tr><tr><td>epoch</td><td>▁▅█</td></tr><tr><td>train_loss</td><td>█▃▁</td></tr><tr><td>val_loss</td><td>█▁▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>auc_score</td><td>0.91411</td></tr><tr><td>epoch</td><td>3</td></tr><tr><td>train_loss</td><td>0.08549</td></tr><tr><td>val_loss</td><td>0.10438</td></tr></table><br/></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["You can sync this run to the cloud by running:<br/><code>wandb sync /home/siu856533724/code/source-code/Social-Networks/Trend-Prediction/Main-Implementation/wandb/offline-run-20231229_202300-vp8vodi2<code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/offline-run-20231229_202300-vp8vodi2/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["# run the sweep\n","wandb.agent(sweep_id, function=trainer, count=6)"]},{"cell_type":"markdown","metadata":{},"source":["## Print the Training loss, Validation loss and AUC values"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["Empty DataFrame\n","Columns: []\n","Index: []"]},"execution_count":33,"metadata":{},"output_type":"execute_result"}],"source":["import pandas as pd\n","\n","# Create a DataFrame from our training statistics.\n","df_stats = pd.DataFrame(data=training_stats)\n","\n","# Display the table.\n","df_stats"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_stats.to_csv(f'/home/siu856533724/code/source-code/Social-Networks/Trend-Prediction/Main-Implementation/logs/emotion_result/train_valid_log_emotion_{get_model_ckpt()}-non-freeze.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["Empty DataFrame\n","Columns: []\n","Index: []"]},"execution_count":35,"metadata":{},"output_type":"execute_result"}],"source":["import pandas as pd\n","\n","# Display floats with two decimal places.\n","# pd.set_option('precision', 2)\n","\n","# Create a DataFrame from our training statistics.\n","df_stats = pd.DataFrame(data=test_stats)\n","\n","# Use the 'epoch' as the row index.\n","# df_stats = df_stats.set_index('Epoch')\n","\n","# A hack to force the column headers to wrap.\n","#df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n","\n","# Display the table.\n","df_stats"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_stats.to_csv(f'/home/siu856533724/code/source-code/Social-Networks/Trend-Prediction/Main-Implementation/logs/emotion_result/test_log_emotion_{get_model_ckpt()}-non-freeze.csv')"]},{"cell_type":"markdown","metadata":{},"source":["## Test BERT model using unknown data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Predicted Outputs: [[0.04972662404179573, 0.05081344395875931, 0.031225554645061493, 0.03223400190472603, 0.043478235602378845, 0.03591315075755119, 0.03896919637918472, 0.03507215157151222, 0.03231365978717804, 0.044278476387262344, 0.02880162186920643, 0.029653994366526604, 0.02706073597073555, 0.0249404925853014, 0.02791152521967888, 0.03434571996331215, 0.022936025634407997, 0.04122310131788254, 0.04890765622258186, 0.028191369026899338, 0.037287529557943344, 0.023911183699965477, 0.03755311295390129, 0.029031742364168167, 0.02581167034804821, 0.030616171658039093, 0.03558940440416336, 0.0722024217247963]]\n"]}],"source":["from utils.bert_classifier import get_model_tokenizer, Tokenize, Classification\n","# Test sentiment prediction\n","test_text = \"I love you\"\n","model, tokenizer = get_model_tokenizer(pred_type='emotion_pred', mode_pt_file='best_emotion_bert-large-uncased-11_12_2023-13_56_57.pt', \n","                                       model_ckpt='bert-large-uncased', labels=28, is_freez=1, gpu='cuda')\n","dict = Tokenize(text=test_text, max_length=30, tokenizer=tokenizer)\n","scores = Classification(dict, model, gpu='cuda')\n","print(f\"Predicted Outputs: {scores}\")"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":4070501,"sourceId":7068668,"sourceType":"datasetVersion"},{"datasetId":4070714,"sourceId":7069029,"sourceType":"datasetVersion"},{"datasetId":4108867,"sourceId":7123201,"sourceType":"datasetVersion"},{"datasetId":4108871,"sourceId":7123207,"sourceType":"datasetVersion"},{"datasetId":4108888,"sourceId":7123228,"sourceType":"datasetVersion"},{"datasetId":4109249,"sourceId":7123742,"sourceType":"datasetVersion"}],"dockerImageVersionId":30588,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"}},"nbformat":4,"nbformat_minor":4}
