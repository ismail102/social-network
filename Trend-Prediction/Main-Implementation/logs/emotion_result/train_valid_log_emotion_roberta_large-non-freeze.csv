,Batch,Max Token,Model,LR,Loss func,Epoch,Train Loss,Valid. Loss,Valid. AUC,F1 score,Recall,Precision,Accuracy
0,16,30,roberta-large,2e-05,bce_loss,1,0.12941035521943198,0.10523448179296986,0.9046060632284503,0.7410238544668817,0.7019409227226476,0.8024010267515357,0.9651340956768542
1,16,30,roberta-large,2e-05,bce_loss,2,0.10182734843928991,0.10089718378325806,0.9200634135854954,0.746150894438314,0.7121159311418768,0.795501651879997,0.9648153099420016
2,16,30,roberta-large,2e-05,bce_loss,3,0.09249009639582348,0.09910498676720848,0.9209593137400114,0.7564804535440134,0.7268182116644728,0.7965460862708986,0.9653346221874871
3,16,70,roberta-large,2e-05,bce_loss,1,0.13146764733150354,0.10411848421492566,0.9107877838396263,0.7468503749598225,0.7141971802995701,0.7933054527617792,0.9646610587799761
4,16,70,roberta-large,2e-05,bce_loss,2,0.10169764376949426,0.10156360560528703,0.915474767683027,0.7535566424559939,0.7253941181859017,0.7910939958783176,0.964733042655588
5,16,70,roberta-large,2e-05,bce_loss,3,0.09234278895943139,0.09933839194365146,0.9216793779650215,0.7552205314881196,0.7253515775896745,0.7957373036705467,0.9652112212578668
6,16,30,roberta-large,2e-05,bce_loss,1,0.12930737277541135,0.10598312728578531,0.9024789592874102,0.7341653525186357,0.692426987047325,0.8036314884060982,0.9649284274608202
7,16,30,roberta-large,2e-05,bce_loss,2,0.10158661729776798,0.10014402234478842,0.9181766938290149,0.7498721125140738,0.7141779597199156,0.802368601054819,0.9655197235819176
8,16,30,roberta-large,2e-05,bce_loss,3,0.09228225819933805,0.09975985048383783,0.921459221342941,0.7538479675857144,0.7247869377195202,0.79297527980083,0.9649130023446176
9,16,70,roberta-large,2e-05,bce_loss,1,0.13175833226751513,0.10595431867708272,0.9052329651418392,0.7377153348030779,0.6981816393545504,0.8007354207345929,0.9648718687014108
10,16,70,roberta-large,2e-05,bce_loss,2,0.10255760074339519,0.10010234799806213,0.9190093892469291,0.7580596643864032,0.7284365865706361,0.7979639597830153,0.9655248652873185
11,16,70,roberta-large,2e-05,bce_loss,3,0.09299039091565311,0.0993469981744594,0.9230375219407781,0.7578013542177657,0.730564293389341,0.7935202465833356,0.9651238122660524
12,16,30,roberta-large,2e-05,bce_loss,1,0.12945775787636132,0.10538623082881281,0.9067404607428067,0.7348380266339174,0.6911890889570673,0.8095371542923253,0.9653551890090906
13,16,30,roberta-large,2e-05,bce_loss,2,0.10101948365157339,0.10060820192895975,0.9211437332960737,0.7544057171342966,0.7282080677270171,0.7885066612174305,0.9645273744395542
14,16,30,roberta-large,2e-05,bce_loss,3,0.09152869427459777,0.09983946402933368,0.9219385238468346,0.7532922605218917,0.7246029918061931,0.7917818053376412,0.9647844597095965
15,16,70,roberta-large,2e-05,bce_loss,1,0.12775226909759474,0.10408060994723724,0.9093768624836152,0.7459852234352768,0.7091970979434634,0.8013171696678836,0.9652677800172761
16,16,70,roberta-large,2e-05,bce_loss,2,0.10085302276486423,0.1013235806201352,0.9178098394898768,0.7468824067947746,0.7109814662600413,0.8001462104395252,0.9652163629632676
17,16,70,roberta-large,2e-05,bce_loss,3,0.09162272885656247,0.09981225638449946,0.9181925647967468,0.7580639765898302,0.7294104255557173,0.7962353473820352,0.9653757558306939
